##
## Copyright (c) 2019 Matthias Tafelmeier.
##
## This file is part of godon
##
## godon is free software: you can redistribute it and/or modify
## it under the terms of the GNU Affero General Public License as
## published by the Free Software Foundation, either version 3 of the
## License, or (at your option) any later version.
##
## godon is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU Affero General Public License for more details.
##
## You should have received a copy of the GNU Affero General Public License
## along with this godon. If not, see <http://www.gnu.org/licenses/>.
##
version: '3.4'
services:
    airflow_web:
        build:
          context: ./
          dockerfile: ./Dockerfile-dask
        restart: always
        env_file:
          - env_vars/airflow_vars.env
          - env_vars/archive_db.env
          - env_vars/meta_db.env
          - env_vars/pals_locking.env
          - env_vars/service_vars.env
        # minimum of setup steps
        entrypoint: bash -c "airflow db init; airflow users create -u airflow -p airflow -r Admin -f airflow -l airflow -e airflow; airflow webserver"
        volumes:
            - ./testing/infra/credentials/ssh/:/opt/airflow/credentials/
            - ./breeder/dags:/opt/airflow/dags/
            - airflow-logs-volume:/opt/airflow/logs/
        ports:
            - 127.0.0.1:8080:8080
    airflow_scheduler:
      build:
        context: ./
        dockerfile: ./Dockerfile-dask
      restart: always
      env_file:
        - env_vars/airflow_vars.env
        - env_vars/archive_db.env
        - env_vars/meta_db.env
        - env_vars/pals_locking.env
        - env_vars/service_vars.env
      entrypoint: bash -c "airflow scheduler"
      volumes:
          - ./testing/infra/credentials/ssh/:/opt/airflow/credentials/
          - ./breeder/dags:/opt/airflow/dags/
          - airflow-logs-volume:/opt/airflow/logs/
      deploy:
        replicas: 2
    meta_data_db:
      hostname: meta-data-db
      image: postgres:13
      environment:
        POSTGRES_USER: meta_data
        POSTGRES_PASSWORD: meta_data
        POSTGRES_DB: meta_data
      volumes:
        - postgres-meta-data-db-volume:/var/lib/postgresql/data
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "meta_data"]
        interval: 5s
        retries: 5
      restart: always
      ports:
         - 127.0.0.7:5432:5432
    airflow_db:
      image: postgres:13
      hostname: airflow-db
      environment:
        POSTGRES_USER: airflow
        POSTGRES_PASSWORD: airflow
        POSTGRES_DB: airflow
      volumes:
          - postgres-db-volume:/var/lib/postgresql/data
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow", "-d", "airflow"]
        interval: 5s
        retries: 5
      restart: always
      ports:
         - 127.0.0.8:5432:5432
    locks_db:
      image: postgres:13
      environment:
        POSTGRES_USER: locking
        POSTGRES_PASSWORD: locking
        POSTGRES_DB: distributed_locking
      volumes:
          - postgres-locking-db-volume:/var/lib/postgresql/data
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "locking", "-d", "distributed_locking"]
        interval: 5s
        retries: 5
      restart: always
      ports:
         - 127.0.0.6:5432:5432
    api:
        build:
          context: ./api/flask/
        image: godon/api:latest
        restart: always
        env_file:
          - env_vars/archive_db.env
          - env_vars/meta_db.env
        environment:
          - AIRFLOW__URL=http://airflow:8080
        volumes:
            - ./breeder/linux_network_stack/:/usr/src/app/openapi_server/templates/
            - ./breeder/dags:/usr/src/app/openapi_server/dags/
        ports:
          - 127.0.0.1:9100:8080
    nats: # queueing system for decoupling dags
        image: nats:2.9.15
        restart: always
        command: "--name nats_server"
        ports:
          - 127.0.0.1:4222:4222
          - 127.0.0.1:8222:8222
    # for airflow engine
    dask_scheduler_airflow:
      build:
          context: ./
          dockerfile: ./Dockerfile-dask
      hostname: dask-scheduler-airflow
      ports:
        - 127.0.0.10:8786:8786
        - 127.0.0.10:8787:8787
      entrypoint: bash -c "dask-scheduler"
    dask_worker_airflow:
      env_file:
        - env_vars/airflow_vars.env
        - env_vars/archive_db.env
        - env_vars/meta_db.env
        - env_vars/pals_locking.env
        - env_vars/service_vars.env
      build:
        context: ./
        dockerfile: ./Dockerfile-dask
      entrypoint: bash -c "dask-worker tcp://dask-scheduler-airflow:8786"
      deploy:
        replicas: 2
      volumes:
        - ./breeder/dags:/opt/airflow/dags/
        - airflow-logs-volume:/opt/airflow/logs/
    # for optuna parallel metaheuristics execution on dask
    dask_scheduler_optuna:
      build:
          context: ./
          dockerfile: ./Dockerfile-dask
      hostname: dask-scheduler-optuna
      ports:
        - 127.0.0.11:8786:8786
        - 127.0.0.11:8787:8787
      entrypoint: bash -c "dask-scheduler"
    dask_worker_optuna:
      env_file:
        - env_vars/airflow_vars.env
        - env_vars/archive_db.env
        - env_vars/meta_db.env
        - env_vars/pals_locking.env
        - env_vars/service_vars.env
      build:
        context: ./
        dockerfile: ./Dockerfile-dask
      entrypoint: bash -c "dask-worker tcp://dask_scheduler:8786"
      deploy:
        replicas: 2
    prometheus:
      image: prom/prometheus
      volumes:
        - ./testing/:/etc/prometheus/
      ports:
          - 9090:9090
    archive_db:
      image: yugabytedb/yugabyte:2.18.1.0-b84
      hostname: archive-db
      environment:
        - "YSQL_DB=archive_db"
      ports:
        - ${DB_YSQL_PORT:-5433}:5433
        - ${DB_YCQL_PORT:-9042}:9042
        # Admin UI
        - ${DB_MASTER_PORT:-7001}:7000
        - ${DB_TSERVER_PORT:-9000}:9000
      command: [
        "bin/yugabyted", "start",
        "--base_dir=/home/yugabyte/yb_data",
        "--daemon=false"
        ]

volumes:
  postgres-db-volume:
  postgres-locking-db-volume:
  postgres-meta-data-db-volume:
  airflow-logs-volume:
